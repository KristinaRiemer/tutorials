---
title: "01-meteorological-data.Rmd"
output: html_document
---

Objectives:

  * This tutorial will walk through the steps required to access meteorological data from the Maricopa Agricultural Center.

Pre-requisites: 

  * Need to have R packages tidyverse, jsonlite, and convertr installed. 
  * Need to have an internet connection. 

## The Maricopa Weather Station

## Meteorological data formats

### Dimensions:

|CF standard-name | units |
|:------------------------------------------|:------|
| time | days since 1970-01-01 00:00:00 UTC|
| longitude | degrees_east|
| latitude |degrees_north|

### Variable names and units

| CF standard-name                          | units | bety         | isimip       | cruncep | narr  | ameriflux |
|:------------------------------------------|:------|:-------------|:-------------|:--------|:------|:----------|
| air_temperature                       | K     | airT         | tasAdjust    | tair    | air   | TA (C)    |
| air_pressure                          | Pa    | air_pressure |              |         |       | PRESS (KPa) |
| mole_fraction_of_carbon_dioxide_in_air    | mol/mol |            |              |         |       | CO2       |
| relative_humidity                         | % | relative_humidity | rhurs       | NA      | rhum  | RH        |
| surface_downwelling_photosynthetic_photon_flux_in_air | mol m-2 s-1 | PAR |     |         |       | PAR *(NOT DONE)*          |
| precipitation_flux                    |  kg m-2 s-1 | cccc   | prAdjust     | rain    | acpc  | PREC (mm/s)          |
|                                           | degrees | wind_direction |          |         |       | WD        |
| wind_speed                                | m/s   | Wspd         |              |         |       | WS        |


* variable names are from [MsTMIP](http://nacp.ornl.gov/MsTMIP_variables.shtml).
* standard_name is CF-convention standard names
* units can be converted by udunits, so these can vary (e.g. the time denominator may change with time frequency of inputs)
* soil moisture for the full column, rather than a layer, is soil_moisture_content

For example, in the [MsTMIP-CRUNCEP](https://www.betydb.org/inputs/280) data, the variable `rain` should be `precipitation_rate`.
We want to standardize the units as well as part of the `met2CF.<product>` step. I believe we want to use the CF "canonical" units but retain the MsTMIP units any time CF is ambiguous about the units.

The key is to process each type of met data (site, reanalysis, forecast, climate scenario, etc) to the exact same standard. This way every operation after that (extract, gap fill, downscale, convert to a model, etc) will always have the exact same inputs. This will make everything else much simpler to code and allow us to avoid a lot of unnecessary data checking, tests, etc being repeated in every downstream function.

### The Geostreams Database

![Schema](https://cloud.githubusercontent.com/assets/9286213/16991300/b2f2b09a-4e60-11e6-96b7-8b63c3d1f995.jpg)

#### Querying the API

The data can be accessed using a URL. You can find data for a particular station, 
a certain sensor from a station, or the datapoints from a sensor. Below are
the URLs for the UA-MAC AZMET weather station, a certain sensor at that station, 
and datapoints from that sensor for the month of January 2017. 

* Station: https://terraref.ncsa.illinois.edu/clowder/api/geostreams/sensors?sensor_name=UA-MAC+AZMET+Weather+Station
* Sensor: https://terraref.ncsa.illinois.edu/clowder/api/geostreams/sensors/438/streams
* Datapoints: https://terraref.ncsa.illinois.edu/clowder/api/geostreams/datapoints?stream_id=46431&since=2017-01-02&until=2017-01-31

All URLs have the same beginning (https://terraref.ncsa.illinois.edu/clowder/api/geostreams), 
then additional information is added for each type of data as shown below. 

* Station: /sensors/sensor_name=[name]
* Sensor: /sensors/[sensor number]/streams
* Datapoints: /datapoints?stream_id=[datapoints number]&since=[start date]&until=[end date]

Possible sensor numbers for a station are found on the page for that station under
"id:", and then datapoints numbers are found on the sensor page under "stream_id:". 

The table belows lists the names of some stations that have available 
meteorological data and associated stream ids. 

| stream id | name                                     |
|------------|------------------------------------------|
| 3211        | UA-MAC AZMET Weather Station - weather  |
| 3212        | UA-MAC AZMET Weather Station - irrigation     |
| 46431        | UA-MAC AZMET Weather Station - weather (5 min)      |
| 3208        | EnvironmentLogger sensor_weather_station |
| 3207        | EnvironmentLogger sensor_par             |
| 748        | EnvironmentLogger sensor_spectrum        |
| 3210        | EnvironmentLogger sensor_co2             |
| 4806       | UIUC Energy Farm SE                      |
| 4807       | UIUC Energy Farm CEN                     |
| 4805       | UIUC Energy Farm NE                      |


Here is the json representation of a single five-minute observation:

```
[
   {
      "geometry":{
         "type":"Point",
         "coordinates":[
            33.0745666667,
            -111.9750833333,
            0
         ]
      },
      "start_time":"2016-08-30T00:06:24-07:00",
      "type":"Feature",
      "end_time":"2016-08-30T00:10:00-07:00",
      "properties":{
         "precipitation_rate":0.0,
         "wind_speed":1.6207870370370374,
         "surface_downwelling_shortwave_flux_in_air":0.0,
         "northward_wind":0.07488770951583902,
         "relative_humidity":26.18560185185185,
         "air_temperature":300.17606481481516,
         "eastward_wind":1.571286062845733,
         "surface_downwelling_photosynthetic_photon_flux_in_air":0.0
      }
   },
```


### Querying weather sensor data stream

The data represent 5 minute summaries aggregated from 1/s observations.

#### Using Curl

First, below is what the API looks like as a URL. Try pasting it into your browser.

https://terraref.ncsa.illinois.edu/clowder/api/geostreams/datapoints?stream_id=46431&since=2017-01-02&until=2017-01-31

These data are for one stream within a chosen time period. They can be 
automatically downloaded into a file on your computer by typing the following
into the command line. This uses the URL from above and the new file is named
spectra.json.

```{sh eval=FALSE}
curl -o spectra.json -X GET https://terraref.ncsa.illinois.edu/clowder/api/geostreams/datapoints?stream_id=46431&since=2017-01-01&until=2017-12-31
```

#### Using R

The following code sets the defaults for showing R code. 
```{r met-setup}
knitr::opts_chunk$set(cache = FALSE, message = FALSE)
```

And this is how you can access the same data in R. This uses the jsonlite R package 
and desired URL to pull the data in. The data is in a dataframe with two nested
dataframes, called `properties` and `geometries`. 

```{r met-geostream}
library(dplyr)
library(ggplot2)
library(jsonlite)
library(lubridate)

weather_all <- fromJSON('https://terraref.ncsa.illinois.edu/clowder/api/geostreams/datapoints?stream_id=46431&since=2017-01-01&until=2017-12-31', flatten = FALSE)
```

The `geometries` dataframe is then pulled out from these data, which contains
the datapoints from this stream. This is combined with a transformed version of the
end of the time period from the stream. 

```{r met-datapoints}
weather_data <- weather_all$properties %>% 
  mutate(time = ymd_hms(weather_all$end_time))
```

### Weather Plots

Create overlaid time series plots for two of the eight variables, wind speed and 
precipitation rate, in the newly created dataframe. 

```{r weather}
theme_set(ggthemes::theme_few())
ggplot(data = weather_data) +
  geom_point(aes(x = time, y = wind_speed), size = 0.7, color = "grey", alpha = 0.5) +
  geom_point(aes(x = time, y = precipitation_rate/2), size = 0.7, color = "red") +
  labs(x = "Date", y = "Speed of wind (m/s)") +
  scale_y_continuous(sec.axis = sec_axis(~.*2, name = "Rate of precipitation (kg/m2s)")) +
  theme(legend.position = c(0.5, 0.5))
```

# High resolution data (1/s) + spectroradiometer

This higher resolution weather data can be used for VNIR calibration, for example. But at 1/s it is very large!

## Lets see how data are downloaded

Here we will download the files using the Clowder API, but note that if you have access to the filesystem (on www.workbench.terraref.org or globus, you can directly access the data in the `sites/ua-mac/Level_1/EnvironmentLogger`. Folder

```{r met-setup2}
knitr::opts_chunk$set(eval = FALSE)
api_url <- "https://terraref.ncsa.illinois.edu/clowder/api"
output_dir <- file.path(tempdir(), "downloads")
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)
```

```{r query-clowder}
library(jsonlite)
library(magrittr)
library(dplyr)
library(RCurl)
library(ggplot2)

# Get Spaces from Clowder - without authentication, result will be Sample Data
spaces <- fromJSON(paste0(api_url, '/spaces'))
print(spaces %>% select(id, name))
```

```{r list-of-datasets}

# Get list of (at most 20) Datasets within that Space from Clowder
datasets <- fromJSON(paste0(api_url, '/spaces/', spaces$id, '/datasets'))
print(datasets %>% select(id, name))
```

```{r list-of-files}
# Get list of Files within any EnvironmentLogger datasets and filter .nc files
files <- fromJSON(paste0(api_url, '/datasets/', datasets$id[grepl("EnvironmentLogger", datasets$name)], '/files'))
ncfiles <- files[grepl('environmentlogger.nc', files$filename), ]
print(ncfiles %>% select(id, filename))
```

## Download netCDF 1/s data from Clowder


```{r nc-download, echo=FALSE}
sources <- paste0(api_url, '/files/', ncfiles$id)
outputs <- paste0(output_dir, ncfiles$filename)

for (i in 1:length(sources)) {
  print(paste0("Downloading ", sources[i], " to ", outputs[i]))
  f <- CFILE(outputs[i], mode = "wb")
  curlPerform(url = sources[i], writedata = f@ref)
  RCurl::close(f)
}
```

### Using the netCDF 1/s data

One use case getting the solar spectrum associated with a particular hyperspectral image.

```{r}
library(ncdf4)
library(ncdf.tools)
library(lubridate)

time <- vector()
vals <- vector()

for (i in 1:length(outputs)) {
  print(paste0("Scanning ", outputs[i]))
  ncfile <- nc_open(outputs[i])
  curr_time <- list()

  metdata <- list()
  for(var in c(names(ncfile$dim), names(ncfile$var))){
    metdata[[var]] <- ncvar_get(ncfile, var)
  }
  lapply(metdata, dim)
  
  days <- ncvar_get(ncfile, varid = "time")
  curr_time <- as.numeric(ymd("1970-01-01") + seconds(days * 24 * 60 * 60))
  
  time <- c(time, curr_time)
  PAR <- c(vals, metdata$`par_sensor/Sensor_Photosynthetically_Active_Radiation`)
}

#ggplot() + 
#  geom_line(aes(time, PAR)) + theme_bw()

print(ncfile)
```

